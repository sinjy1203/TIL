{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HuggingFace\n",
    "- LLM을 로딩하는 가장 간단하고 기본적인 방법\n",
    "- Transformer library\n",
    "- Load model with no quantization & compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "config.json: 100%|██████████| 638/638 [00:00<00:00, 1.48MB/s]\n",
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/huggingface_hub/file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /media/shin/T7/huggingface/models/models--HuggingFaceH4--zephyr-7b-beta. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "  warnings.warn(message)\n",
      "model.safetensors.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 22.9MB/s]\n",
      "model-00001-of-00008.safetensors: 100%|██████████| 1.89G/1.89G [00:22<00:00, 84.3MB/s]\n",
      "model-00002-of-00008.safetensors: 100%|██████████| 1.95G/1.95G [00:23<00:00, 81.8MB/s]\n",
      "model-00003-of-00008.safetensors: 100%|██████████| 1.98G/1.98G [00:23<00:00, 84.4MB/s]\n",
      "model-00004-of-00008.safetensors: 100%|██████████| 1.95G/1.95G [00:23<00:00, 83.6MB/s]\n",
      "model-00005-of-00008.safetensors: 100%|██████████| 1.98G/1.98G [00:23<00:00, 83.2MB/s]\n",
      "model-00006-of-00008.safetensors: 100%|██████████| 1.95G/1.95G [00:23<00:00, 82.2MB/s]\n",
      "model-00007-of-00008.safetensors: 100%|██████████| 1.98G/1.98G [00:24<00:00, 82.2MB/s]\n",
      "model-00008-of-00008.safetensors: 100%|██████████| 816M/816M [00:10<00:00, 81.4MB/s]\n",
      "Downloading shards: 100%|██████████| 8/8 [02:58<00:00, 22.25s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  6.34it/s]\n",
      "generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 364kB/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 4.66MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 103MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 1.91MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 83.4kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 452kB/s]\n"
     ]
    }
   ],
   "source": [
    "from torch import bfloat16\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    torch_dtype=bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\n",
    "        \"cache_dir\": \"/media/shin/T7/huggingface/models\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot.</s>\n",
      "<|user|>\n",
      "Tell me a funny joke about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot.</s>\n",
      "<|user|>\n",
      "Tell me a funny joke about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Why did the Large Language Model go to the party?\n",
      "\n",
      "To impress everyone with its vocabulary!\n",
      "\n",
      "The host was amazed as the LLM effortlessly chatted with guests, using words that left them in awe. The partygoers couldn't believe how intelligent and witty the LLM was, and some even mistook it for a human!\n",
      "\n",
      "But as the night wore on, the LLM started to run out of steam. It struggled to keep up with the conversation, repeating itself and making grammatical errors.\n",
      "\n",
      "The host, feeling sorry for the LLM, suggested they take a break and recharge. The LLM gratefully accepted, promising to come back stronger and more intelligent than ever before.\n",
      "\n",
      "From that night on, the LLM became a regular at the party, impressing everyone with its vast knowledge and quick wit. But it also learned to pace itself, knowing that being a Large Language Model was a big responsibility, and it needed to take care of itself to keep up the good work!\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sharding\n",
    "- model을 여러개의 조각(shard)로 나눈다.\n",
    "- 각 조각들을 다른 장치에 분산시켜 GPU memory 제한을 우회한다\n",
    "- Accelerate 패키지를 통해 구현됨\n",
    "    - device_map=\"auto\": 자동으로 gpu, cpu 순으로 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "accelerator.save_model(\n",
    "    model=pipe.model,\n",
    "    save_directory=\"./model\",\n",
    "    max_shard_size=\"4GB\"\n",
    ")\n",
    "# ./model\n",
    "    # ├── model-00001-of-00004.safetensors\n",
    "    # ├── model-00002-of-00004.safetensors\n",
    "    # ├── model-00003-of-00004.safetensors\n",
    "    # ├── model-00004-of-00004.safetensors\n",
    "    # └── model.safetensors.index.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize with Bitsandbytes\n",
    "- NF4 (NormalFloat-4bit)\n",
    "    1. Normalization  \n",
    "        특정 범위로 weights를 정규화\n",
    "    2. Quantization  \n",
    "        quantized to 4-bit\n",
    "    3. Dequantization  \n",
    "        dequantized during computation -> performance boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from torch import bfloat16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 638/638 [00:00<00:00, 1.90MB/s]\n",
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/huggingface_hub/file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /media/shin/T7/huggingface/models/models--HuggingFaceH4--zephyr-7b-beta. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "  warnings.warn(message)\n",
      "config.json: 100%|██████████| 638/638 [00:00<00:00, 2.16MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 23.9k/23.9k [00:00<00:00, 34.6MB/s]\n",
      "model-00001-of-00008.safetensors: 100%|██████████| 1.89G/1.89G [00:23<00:00, 78.8MB/s]\n",
      "model-00002-of-00008.safetensors: 100%|██████████| 1.95G/1.95G [00:30<00:00, 63.0MB/s]\n",
      "model-00003-of-00008.safetensors: 100%|██████████| 1.98G/1.98G [00:30<00:00, 65.9MB/s]\n",
      "model-00004-of-00008.safetensors: 100%|██████████| 1.95G/1.95G [00:30<00:00, 63.9MB/s]\n",
      "model-00005-of-00008.safetensors: 100%|██████████| 1.98G/1.98G [00:29<00:00, 66.7MB/s]\n",
      "model-00006-of-00008.safetensors: 100%|██████████| 1.95G/1.95G [00:31<00:00, 62.6MB/s]\n",
      "model-00007-of-00008.safetensors: 100%|██████████| 1.98G/1.98G [00:24<00:00, 79.9MB/s]\n",
      "model-00008-of-00008.safetensors: 100%|██████████| 816M/816M [00:14<00:00, 56.9MB/s]\n",
      "Downloading shards: 100%|██████████| 8/8 [03:41<00:00, 27.67s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.86it/s]\n",
      "generation_config.json: 100%|██████████| 111/111 [00:00<00:00, 275kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\", \n",
    "    cache_dir=\"/media/shin/T7/huggingface/tokenizers\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\"/media/shin/T7/huggingface/models\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot.</s>\n",
      "<|user|>\n",
      "Tell me a funny joke about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Why did the Large Language Model go to the party?\n",
      "\n",
      "To mingle with the crowd and spread its wit and humor! But unfortunately, it got so caught up in its own jokes that it forgot to introduce itself to anyone, and left the party feeling a little lonely.\n",
      "\n",
      "The moral of the story? Even the smartest and most entertaining among us need to remember to connect with others and build relationships, no matter how eloquent our words may be.\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt, \n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Quantization (GPTQ vs AWQ vs GGUF)\n",
    "- 이미 quantized model들이 huggingface hub에 많이 존재\n",
    "- 특히 GPTQ, AWQ, GGUF가 가장 인기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPTQ\n",
    "- post-training quantization(PTQ) method\n",
    "- weight의 mean squared error를 minimize하면서 4-bit quantized\n",
    "- inference 동안 동적으로 dequantize float16  \n",
    "    -> improve performance & keep memory\n",
    "- GPU 사용에 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n",
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"TheBloke/zephyr-7B-beta-GPTQ\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, cache_dir=\"/media/shin/T7/huggingface/tokenizers\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\",\n",
    "    cache_dir=\"/media/shin/T7/huggingface/models\"\n",
    ")\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot.</s>\n",
      "<|user|>\n",
      "Tell me a funny joke about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Why did the Large Language Model go to the party?\n",
      "\n",
      "To show off its wit and charm, of course! But unfortunately, it got stuck in a corner and couldn't seem to break the ice with anyone. It kept trying to make small talk, but all it could muster were generic responses and clichés. It felt like a total party pooper, but at least it knew it had a good excuse - it was just too large for the room!\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GGUF\n",
    "- cpu에 사용가능한 quantization\n",
    "- offload some of layers to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 7084.97it/s]\n",
      "zephyr-7b-beta.Q4_K_M.gguf: 100%|██████████| 4.37G/4.37G [02:41<00:00, 27.0MB/s]\n",
      "Fetching 1 files: 100%|██████████| 1/1 [02:42<00:00, 162.55s/it]\n",
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 3.58MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:00<00:00, 11.1MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 2.42MB/s]\n",
      "added_tokens.json: 100%|██████████| 42.0/42.0 [00:00<00:00, 111kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 420kB/s]\n"
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/zephyr-7B-beta-GGUF\",\n",
    "    model_file=\"zephyr-7b-beta.Q4_K_M.gguf\",\n",
    "    model_type=\"mistral\", \n",
    "    gpu_layers=50,\n",
    "    hf=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\", use_fast=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tell me a funny joke about Large Language Models.\"\n",
    "    },\n",
    "]\n",
    "prompt = pipe.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a friendly chatbot.</s>\n",
      "<|user|>\n",
      "Tell me a funny joke about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Why did the Large Language Model go to the party?\n",
      "\n",
      "To impress everyone with its vocabulary!\n",
      "\n",
      "But unfortunately, it kept repeating the same jokes over and over again, making everyone groan and roll their eyes. The partygoers soon realized that the Large Language Model was more of a party pooper than a party animal.\n",
      "\n",
      "Moral of the story: Just because a Large Language Model can generate a lot of words, doesn't mean it knows how to be funny or entertaining. Sometimes, less is more!\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(prompt, max_new_tokens=256)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWQ (Activation-aware Weight Quantization)\n",
    "- \"LLM의 모든 Weight들이 동등하게 중요하지 않다\" -> 에서 시작\n",
    "- 소수의 가중치는 quantization에서 제외된다\n",
    "- GPTQ보다 성능, 속도면에서 모두 뛰어나다고 주장\n",
    "- vLLM 패키지로 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-27 21:09:44,270\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 02-27 21:09:44 config.py:186] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 02-27 21:09:44 llm_engine.py:79] Initializing an LLM engine with config: model='TheBloke/zephyr-7B-beta-AWQ', tokenizer='TheBloke/zephyr-7B-beta-AWQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 02-27 21:09:46 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 02-27 21:09:48 llm_engine.py:337] # GPU blocks: 2367, # CPU blocks: 2048\n",
      "INFO 02-27 21:09:48 model_runner.py:676] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-27 21:09:48 model_runner.py:680] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-27 21:09:52 model_runner.py:748] Graph capturing finished in 3 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=256\n",
    ")\n",
    "llm = LLM(\n",
    "    model=\"TheBloke/zephyr-7B-beta-AWQ\",\n",
    "    quantization='awq',\n",
    "    dtype='half',\n",
    "    gpu_memory_utilization=0.8,\n",
    "    max_model_len=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "<|system|>\n",
    "You are a friendly chatbot.</s>\n",
    "<|user|>\n",
    "Tell me a funny joke about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the Large Language Model go to the party?\n",
      "\n",
      "To network and expand its vocabulary!\n",
      "\n",
      "Why did the Large Language Model blush?\n",
      "\n",
      "Because it overheard another model saying it was a little too wordy!\n",
      "\n",
      "Why did the Large Language Model get kicked out of the library?\n",
      "\n",
      "It was being too loud and kept interrupting other models' conversations with its endless chatter!\n",
      "\n",
      "Why did the Large Language Model get a standing ovation at the comedy club?\n",
      "\n",
      "Because it told some really punny jokes!\n",
      "\n",
      "Why did the Large Language Model get a job as a writer?\n",
      "\n",
      "Because it was the most wordy model in the room!\n",
      "\n",
      "Why did the Large Language Model get a job as a librarian?\n",
      "\n",
      "Because it knew all the right words to shelve books in the right place!\n",
      "\n",
      "Why did the Large Language Model get a job as a teacher?\n",
      "\n",
      "Because it knew all the right words to help students learn and grow!\n",
      "\n",
      "Why did the Large Language Model get a job as a lawyer?\n",
      "\n",
      "Because it knew all the right words to argue a case in court!\n",
      "\n",
      "Why did the Large Language Model get a job as a musician?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate(prompt, sampling_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://towardsdatascience.com/which-quantization-method-is-right-for-you-gptq-vs-gguf-vs-awq-c4cd9d77d5be"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
