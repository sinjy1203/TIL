{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPTQ - 4 bit Quantization\n",
    "- OBQ 기반 quantization 알고리즘\n",
    "- Arbitrary order insight, Lazy Batch-Updates, Cholesky Reformulation 기법 사용  \n",
    "    -> 큰 언어모델에서도 효율적으로 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Define base model and output directory\n",
    "model_id = \"gpt2\"\n",
    "out_dir = \"/media/shin/T7/model_ckpt/\" + model_id + \"-GPTQ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model & tokenizer with configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=128, # lazy batch size\n",
    "    damp_percent=0.01, # Cholesky reformulation을 위한 값\n",
    "    desc_act=False # 가중치 영향력 고려한 order -> True: 정확도 향상, 속도 하락\n",
    ")\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_id, quantize_config=quantize_config, cache_dir=\"/media/shin/T7/huggingface/models\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"/media/shin/T7/huggingface/tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize with samples\n",
    "- 원본 모델과 quantized 모델의 출력 사이의 비교수단\n",
    "- 샘플 수가 많을수록 quantization quality 상승"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 318M/318M [02:03<00:00, 2.58MB/s]]\n",
      "Generating train split: 356318 examples [00:00, 419675.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1024\n",
    "data = load_dataset(\n",
    "    \"allenai/c4\", \n",
    "    data_files=\"en/c4-train.00001-of-01024.json.gz\", \n",
    "    split=f\"train[:{n_samples*5}]\", \n",
    "    cache_dir=\"/media/shin/T7/huggingface/datasets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2441065 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = tokenizer(\"\\n\\n\".join(data[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_ids = []\n",
    "for _ in range(n_samples):\n",
    "    i = random.randint(0, tokenized_data.input_ids.shape[1] - tokenizer.model_max_length - 1)\n",
    "    j = i + tokenizer.model_max_length\n",
    "    input_ids = tokenized_data.input_ids[:, i:j]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    examples_ids.append({'input_ids': input_ids, 'attention_mask': attention_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.quantize(\n",
    "    examples_ids,\n",
    "    batch_size=1,\n",
    ")\n",
    "model.save_quantized(out_dir, use_safetensors=True)\n",
    "tokenizer.save_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loaded quantized model & generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - ignoring unknown parameter in quantize_config.json: quant_method.\n",
      "INFO - The layer lm_head is not quantized.\n",
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "WARNING - GPT2GPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "WARNING - GPT2GPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Reload model and tokenizer\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    out_dir,\n",
    "    device=device,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPT2GPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a dream and can't believe it has arrived!\" –Liam MacCulff in Anzac Days \"If one does not understand why in a thousand years we were born, what is life? A life of sorrow... A life of\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "result = generator(\"I have a dream\", do_sample=True, max_length=50)[0]['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
