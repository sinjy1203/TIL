{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive 8-bit Quantization\n",
    "- absolute maximum (absmax) quantization\n",
    "- zero-point quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute maximum (absmax) Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def absmax_quantize(X):\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "    \n",
    "    X_quant = torch.round(scale * X)\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7168, -2.2098,  0.1761],\n",
       "         [-1.6349, -0.7506, -0.8688],\n",
       "         [ 1.7245,  0.1274, -0.3302]]),\n",
       " tensor([[  41, -127,   10],\n",
       "         [ -94,  -43,  -50],\n",
       "         [  99,    7,  -19]], dtype=torch.int8),\n",
       " tensor([[ 0.7134, -2.2098,  0.1740],\n",
       "         [-1.6356, -0.7482, -0.8700],\n",
       "         [ 1.7226,  0.1218, -0.3306]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.randn((3, 3))\n",
    "weights_quant, weights_dequant = absmax_quantize(weights)\n",
    "weights, weights_quant, weights_dequant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-point Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropoint_quantize(X):\n",
    "    x_range = torch.max(X) - torch.min(X)\n",
    "    x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "    scale = 255 / x_range\n",
    "    zeropoint = torch.round(-scale * torch.min(X) - 128)\n",
    "\n",
    "    X_quant = torch.clip(torch.round(X * scale + zeropoint), -128, 127)\n",
    "    X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2.3903,  0.2432,  1.2972],\n",
       "         [ 0.9187,  1.7886,  0.2253],\n",
       "         [-0.0748, -0.2997,  0.6005]]),\n",
       " tensor([[ 127,  -77,   23],\n",
       "         [ -13,   70,  -79],\n",
       "         [-107, -128,  -43]], dtype=torch.int8),\n",
       " tensor([[ 2.3947,  0.2426,  1.2975],\n",
       "         [ 0.9178,  1.7934,  0.2215],\n",
       "         [-0.0738, -0.2954,  0.6013]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.randn((3, 3)) + 0.5\n",
    "weights_quant, weights_dequant = zeropoint_quantize(weights)\n",
    "weights, weights_quant, weights_dequant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using transformers library\n",
    "- GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510342192\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=\"/media/shin/T7/huggingface/models\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=\"/media/shin/T7/huggingface/tokenizers\")\n",
    "\n",
    "print(model.get_memory_footprint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test GPT-2 first attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
      "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
      "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
      "        ...,\n",
      "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
      "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
      "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[-21, -12,  -4,  ...,   2,  -3,   1],\n",
      "        [  4,   7,  11,  ...,  -2,  -1,  -1],\n",
      "        [  0,   3,  16,  ...,   5,   2,  -1],\n",
      "        ...,\n",
      "        [-12,  -1,   9,  ...,   0,  -2,   1],\n",
      "        [  7,  10,   5,  ...,   1,  -2,  -2],\n",
      "        [-18,  -9, -11,  ...,   0,   0,   1]], dtype=torch.int8)\n",
      "\n",
      "Zero-point quantized weights:\n",
      "tensor([[-20, -11,  -3,  ...,   3,  -2,   2],\n",
      "        [  5,   8,  12,  ...,  -1,   0,   0],\n",
      "        [  1,   4,  18,  ...,   6,   3,   0],\n",
      "        ...,\n",
      "        [-11,   0,  10,  ...,   1,  -1,   2],\n",
      "        [  8,  11,   6,  ...,   2,  -1,  -1],\n",
      "        [-18,  -8, -10,  ...,   1,   1,   2]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "weights = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, _ = absmax_quantize(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_zp_quant, _ = zeropoint_quantize(weights)\n",
    "print(\"\\nZero-point quantized weights:\")\n",
    "print(weights_zp_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### quantization all layers of GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Store original weights\n",
    "weights = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create model to quantize\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_abs = []\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)\n",
    "\n",
    "# Create model to quantize\n",
    "model_zp = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_zp = []\n",
    "for param in model_zp.parameters():\n",
    "    _, dequantized = zeropoint_quantize(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_zp.append(dequantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream,\" he said. \"You work as long as you can. No excuses. I want to be someone that I can make an impact and do things I enjoy and it gives me a big motivation to keep doing what I'm doing\n",
      "--------------------------------------------------\n",
      "Absmax model:\n",
      "I have a dream on my hands, to turn my knees on you, or something else; and I will soon get to that point.\" (Lying down) \"If that is possible--then I'll use it. (Pause.)\" *\n",
      "--------------------------------------------------\n",
      "Zeropoint model:\n",
      "I have a dream for you at the moment.\n",
      "\n",
      "It is a dream to share an intimate home with another human to spend their nights away from one's home while simultaneously exploring and enjoying both life and art. We see the same kind of art\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=input_ids.new_ones(input_ids.shape)\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with original and quantized models\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "absmax_text   = generate_text(model_abs, \"I have a dream\")\n",
    "zp_text       = generate_text(model_zp, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate with perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    \n",
    "    neg_log_likelihood = outputs.loss\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  12.41\n",
      "Absmax perplexity:    32.97\n",
      "Zeropoint perplexity: 31.69\n"
     ]
    }
   ],
   "source": [
    "ppl     = calculate_perplexity(model, original_text)\n",
    "ppl_abs = calculate_perplexity(model_abs, absmax_text)\n",
    "ppl_zp  = calculate_perplexity(model_zp, absmax_text)\n",
    "\n",
    "print(f\"Original perplexity:  {ppl.item():.2f}\")\n",
    "print(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\n",
    "print(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-bit Quantization with LLM.int8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 176,527,896 bytes\n"
     ]
    }
   ],
   "source": [
    "model_id = \"gpt2\"\n",
    "\n",
    "model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map='auto',  # gpu, cpu 순으로 최대한 load\n",
    "    load_in_8bit= True, # 734MB -> 418MB\n",
    "    cache_dir=\"/media/shin/T7/huggingface/models\"\n",
    ")\n",
    "print(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### original vs LLM.int8() by perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shin/anaconda3/envs/transformer/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream,\" he said. \"You work as long as you can. No excuses. I want to be someone that I can make an impact and do things I enjoy and it gives me a big motivation to keep doing what I'm doing\n",
      "--------------------------------------------------\n",
      "LLM.int8() model:\n",
      "I have a dream that will continue to be fulfilled. That's why I believe we need to bring back this legislation and that's why I strongly oppose it.\n",
      "\n",
      "We have to bring back any illegal immigration. Even if we end up as part\n"
     ]
    }
   ],
   "source": [
    "text_int8 = generate_text(model_int8, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"LLM.int8() model:\\n{text_int8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (original):   12.41\n",
      "Perplexity (LLM.int8()): 14.87\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity (original):   {ppl.item():.2f}\")\n",
    "\n",
    "ppl = calculate_perplexity(model_int8, text_int8)\n",
    "print(f\"Perplexity (LLM.int8()): {ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
