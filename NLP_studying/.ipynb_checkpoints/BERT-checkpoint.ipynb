{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241c81e1",
   "metadata": {},
   "source": [
    "# NLP에서의 사전훈련 (Pre-training)\n",
    "---\n",
    "\n",
    "## 사전 훈련된 워드 임베딩\n",
    "- 임베딩 방법\n",
    "    1. 임베딩 층을 처음부터 학습\n",
    "    2. 사전 학습된 임베딩 벡터들을 가져와 사용\\\n",
    "        => 데이터 적을 때\n",
    "- 이 두 방법들은 하나의 단어의 하나의 벡터로 맵핑\\\n",
    "    => 다의어나 동음이의어 구분 X\n",
    "- 이 문제 해결하기 위해 => 사전훈련된 언어모델 (ELMo, BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fe400",
   "metadata": {},
   "source": [
    "## 사전 훈련된 언어 모델\n",
    "![](https://wikidocs.net/images/page/108730/image1.PNG)\n",
    "- 학습 방법\n",
    "1. 라벨링 없는 데이터셋을 통해 다음 단어를 예측하는 LSTM 언어모델을 사전훈련\n",
    "2. 그 LSTM모델을 이용해서 실제 task에 파인튜닝\n",
    "\n",
    "- 사전 훈련없이 바로 실제 task에 훈련시키는 것보다 위의 방법이 더 효과 업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0e295",
   "metadata": {},
   "source": [
    "## ELMO\n",
    "![](https://wikidocs.net/images/page/108730/image2.PNG)\n",
    "1. 순방향 언어 모델과 역방향 언어 모델을 따로 학습\n",
    "2. 언어 모델로 부터 임베딩 값 획득\\\n",
    "=> 문맥에 따라 임베딩 벡터 값이 달라짐 굳!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8aa1f",
   "metadata": {},
   "source": [
    "## GPT-1\n",
    "![](https://wikidocs.net/images/page/108730/image3.PNG)\n",
    "- Trm: 트랜스포머 디코더\n",
    "- 디코더 12개층 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e39e9",
   "metadata": {},
   "source": [
    "## 마스크드 언어 모델 (Masked Language Model)\n",
    "- 입력 텍스트의 단어 집합의 15%를 랜덤 마스킹\n",
    "- 인공 신경망에게 마스킹된 단어들을 예측하게 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d43a18",
   "metadata": {},
   "source": [
    "## BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- 구글이 공개한 사전 훈련 모델\n",
    "![](https://wikidocs.net/images/page/35594/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b77bc1",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c86305",
   "metadata": {},
   "source": [
    "### BERT의 문맥을 반영한 임베딩 (Contextual Embedding)\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC2.PNG)\n",
    "- BERT의 입력 임베딩은 단순 임베딩 층을 거친 벡터\n",
    "- 출력임베딩은 BERT를 지나고 나서 모든 입력 단어를 참고한후 출력\n",
    "- 출력임베딩과 입력임베딩은 동일한 차원\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca05496",
   "metadata": {},
   "source": [
    "### BERT의 서브워드 토크나이저: WordPiece\n",
    "- 바이트 페어 인코딩(BPE)의 유사 알고리즘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
