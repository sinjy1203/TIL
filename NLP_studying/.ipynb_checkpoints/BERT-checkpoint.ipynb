{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "241c81e1",
   "metadata": {},
   "source": [
    "# NLP에서의 사전훈련 (Pre-training)\n",
    "---\n",
    "\n",
    "## 사전 훈련된 워드 임베딩\n",
    "- 임베딩 방법\n",
    "    1. 임베딩 층을 처음부터 학습\n",
    "    2. 사전 학습된 임베딩 벡터들을 가져와 사용\\\n",
    "        => 데이터 적을 때\n",
    "- 이 두 방법들은 하나의 단어의 하나의 벡터로 맵핑\\\n",
    "    => 다의어나 동음이의어 구분 X\n",
    "- 이 문제 해결하기 위해 => 사전훈련된 언어모델 (ELMo, BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fe400",
   "metadata": {},
   "source": [
    "## 사전 훈련된 언어 모델\n",
    "![](https://wikidocs.net/images/page/108730/image1.PNG)\n",
    "- 학습 방법\n",
    "1. 라벨링 없는 데이터셋을 통해 다음 단어를 예측하는 LSTM 언어모델을 사전훈련\n",
    "2. 그 LSTM모델을 이용해서 실제 task에 파인튜닝\n",
    "\n",
    "- 사전 훈련없이 바로 실제 task에 훈련시키는 것보다 위의 방법이 더 효과 업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0e295",
   "metadata": {},
   "source": [
    "## ELMO\n",
    "![](https://wikidocs.net/images/page/108730/image2.PNG)\n",
    "1. 순방향 언어 모델과 역방향 언어 모델을 따로 학습\n",
    "2. 언어 모델로 부터 임베딩 값 획득\\\n",
    "=> 문맥에 따라 임베딩 벡터 값이 달라짐 굳!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8aa1f",
   "metadata": {},
   "source": [
    "## GPT-1\n",
    "![](https://wikidocs.net/images/page/108730/image3.PNG)\n",
    "- Trm: 트랜스포머 디코더\n",
    "- 디코더 12개층 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176e39e9",
   "metadata": {},
   "source": [
    "## 마스크드 언어 모델 (Masked Language Model)\n",
    "- 입력 텍스트의 단어 집합의 15%를 랜덤 마스킹\n",
    "- 인공 신경망에게 마스킹된 단어들을 예측하게 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d43a18",
   "metadata": {},
   "source": [
    "# BERT (Bidirectional Encoder Representations from Transformers)\n",
    "- 구글이 공개한 사전 훈련 모델\n",
    "![](https://wikidocs.net/images/page/35594/%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b77bc1",
   "metadata": {},
   "source": [
    "![](https://wikidocs.net/images/page/35594/bartbase%EC%99%80large.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c86305",
   "metadata": {},
   "source": [
    "## BERT의 문맥을 반영한 임베딩 (Contextual Embedding)\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC2.PNG)\n",
    "- BERT의 입력 임베딩은 단순 임베딩 층을 거친 벡터\n",
    "- 출력임베딩은 BERT를 지나고 나서 모든 입력 단어를 참고한후 출력\n",
    "- 출력임베딩과 입력임베딩은 동일한 차원\n",
    "\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca05496",
   "metadata": {},
   "source": [
    "## BERT의 서브워드 토크나이저: WordPiece\n",
    "- 바이트 페어 인코딩(BPE)의 유사 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265531d",
   "metadata": {},
   "source": [
    "## 포지션 임베딩 (Position Embedding)\n",
    "- 트랜스포머에서는 사인 코사인 함수를 통해 얻고 단어 임베딩과 더함\n",
    "- BERT는 학습을 통해 얻는 포지션 임베딩 사용\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG)\n",
    "- BERT는 문장의 최대길이가 512 이므로 512개의 포지션 벡터를 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f03b8",
   "metadata": {},
   "source": [
    "## BERT의 사전 훈련 (Pre-training)\n",
    "![](https://wikidocs.net/images/page/35594/bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png)\n",
    "- BERT는 학습방법으로 마스크드 언어모델, 다음문장 예측\n",
    "\n",
    "\n",
    "### 마스크드 언어 모델 (Masked Language Model, MLM)\n",
    "- 입력 텍스트의 15%의 단어를 처리\n",
    "- 80%는 마스크로 변경하고 10%는 단어 변경, 나머지는 그대로\\\n",
    "    => 마스크만 할 경우 실제 예측에서는 마스크가 없기에 사전 학습단계와 파인튜닝 단계의 입력 데이터 불일치 발생\n",
    "- 입력 텍스트의 15% 단어만 학습에 사용\\\n",
    "아래 그림에서는 마스킹, 변경, 유지 세가지를 실행한 세개의 단어만 학습 \n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC9.PNG)\n",
    "\n",
    "### 다음 문장 예측 (Next Sentence Prediction, NSP)\n",
    "- 두개의 문장을 주고 이어지는 문장인지 아닌지 학습\n",
    "- 두 문장을 입력으로 받는데 각 문장의 끝에 [SEP]토큰을 넣는다.\n",
    "- 가장 처음에 [CLS]토큰을 넣고 그 부분에 이진분류 학습\\\n",
    "=> 문장 간의 관계를 이해해야 하는 테스크에 활용하기 위해\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC10.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80afa87",
   "metadata": {},
   "source": [
    "## 세그먼트 임베딩 (Segment Embedding)\n",
    "- 문장구분을 위해서 세그먼트 임베딩층 추가\n",
    "- 첫번째 문장은 0 임베딩, 두번재 문장은 1 임베딩을 더해줌 (임베딩 벡터는 두개만 사용)\n",
    "- 단 두 문장이 두 문서등로 종류를 자유롭게 할 수도 있고 관계가 필요없으면 임베딩 0으로만 구성할 수 도 있다\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9255ddc4",
   "metadata": {},
   "source": [
    "## BERT의 3개의 임베딩층\n",
    "- WordPiece Embedding\n",
    "    - 벡터의 종류는 단어집합크기 30522개\n",
    "- Position Embedding\n",
    "    - 벡터의 종류는 최대 문장길이 512개\n",
    "- Segment Embedding\n",
    "    - 벡터의 종류는 구분을 위한 2개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32332b2c",
   "metadata": {},
   "source": [
    "## BERT를 파인 튜닝(Fine-tuning) 하기\n",
    "- 사전 학습된 BERT에 목표 task의 데이터를 추가로 학습하는 단계\n",
    "\n",
    "\n",
    "### 하나의 텍스트에 대한 텍스트 분류 유형 (Single Text Classification)\n",
    "- 문서 시작에 [CLS]을 입력하여 그 부분에 Dense를 추가하여 분류 예측\n",
    "![](https://wikidocs.net/images/page/115055/apply1.PNG)\n",
    "\n",
    "\n",
    "### 하나의 텍스트에 대한 태깅 작업 (Tagging)\n",
    "- 입력 텍스트 위치의 Dense를 추가하여 분류 예측\n",
    "![](https://wikidocs.net/images/page/115055/apply2.PNG)\n",
    "\n",
    "\n",
    "### 텍스트의 쌍에 대한 분류 또는 회귀 문제 (Text Pair Classification or Regression)\n",
    "- 텍스트 쌍을 입력받는 태스크\n",
    "ex) 자연어 추론(Natural language inference): 텍스트간 관계 예측\n",
    "![](https://wikidocs.net/images/page/115055/apply3.PNG)\n",
    "\n",
    "\n",
    "### 질의 응답 (Question Answering)\n",
    "- 질문과 본문을 입력받으면 본문의 일부분을 추출해서 답변\n",
    "![](https://wikidocs.net/images/page/115055/apply4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9f980",
   "metadata": {},
   "source": [
    "## 어텐션 마스크 (Attention Mask)\n",
    "- 어텐션 마스크 입력 추가로 필요\n",
    "- 패딩 토큰에 어텐션 하지 않도록 하기위해\n",
    "- 1은 실제 단어 0은 패딩토큰\n",
    "![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC11.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0175a",
   "metadata": {},
   "source": [
    "### BERT의 문장 임베딩\n",
    "- BERT로 부터 문장 벡터를 얻는 방법 세가지\n",
    "1. [CLS]토큰의 출력벡터를 문장벡터로 간주\n",
    "    => 문장 분류문제에 사용했음\n",
    "![](https://wikidocs.net/images/page/156176/%EA%B7%B8%EB%A6%BC1.PNG)\n",
    "2. BERT의 모든 출력 벡터들의 평균을 문장벡터로 간주\n",
    "3. 2번의 평균대신 맥스 풀링 사용\n",
    "![](https://wikidocs.net/images/page/156176/%EA%B7%B8%EB%A6%BC2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16355ef4",
   "metadata": {},
   "source": [
    "## 센텐스버트 (Sentence BERT, SBERT)\n",
    "- BERT의 문장임베딩 성능 개선 모델\n",
    "- 위의 문장임베딩을 파인튜닝\n",
    "\n",
    "\n",
    "### 문장 쌍 분류 태스크로 파인 튜닝\n",
    "- 두 문장의 관계를 맞추는 문제를 학습\n",
    "- 각 문장을 BERT를 통해 문장임베딩을 획득\n",
    "- 두 문장임베딩과 mae를 분류문제로 학습\\\n",
    "u, v 가 n차원 이면 dense 입력은 3n차원\n",
    "![](https://wikidocs.net/images/page/156176/%EA%B7%B8%EB%A6%BC4.PNG)\n",
    "\n",
    "\n",
    "### 문장 쌍 회귀 태스크로 파인 튜닝\n",
    "- 두개의 문장으로 의미적 유사성을 구하는 문제 학습\n",
    "    => 결과 범위는 0 ~ 5\n",
    "- 두 문장의 BERT 문장 임베딩을 얻고 두 벡터의 코사인 유사도 계산\n",
    "- 라벨과의 mse를 통한 학습\n",
    "![](https://wikidocs.net/images/page/156176/%EA%B7%B8%EB%A6%BC5.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
