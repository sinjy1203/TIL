{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3047d7",
   "metadata": {},
   "source": [
    "# 카운트 기반의 단어 표현(Count based word Representation\n",
    "텍스트를 표현하는 방법\\\n",
    "단어의 중요정도, 핵심어 추출, 문서들 간의 유사도 측정으로 활용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a6228",
   "metadata": {},
   "source": [
    "## 단어의 표현 방법\n",
    "- **국소 표현(Local Representation = Discrete Representation)**\\\n",
    "해당 단어만 보고 특정 값 매칭 => 단어의 의미, 뉘앙스 표현 x\n",
    "- **분산 표현(Distributed Representation = Continuous Representation)**\\\n",
    "주변을 참고하여 단어 표현 => 단어의 의미, 뉘앙스 표현 o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0145c9fa",
   "metadata": {},
   "source": [
    "![nn](https://wikidocs.net/images/page/31767/wordrepresentation.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a37c5e8",
   "metadata": {},
   "source": [
    "## Bag of Words(BoW)\n",
    "단어의 등장 순서 고려x\\\n",
    "빈도수 기반\n",
    "\n",
    "=> 문서에 등장하는 각 단어의 등장 횟수를 구한다\\\n",
    "=> 어떤 성격의 문서인지 판단하는 작업에 쓰임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eeae9a",
   "metadata": {},
   "source": [
    "**BoW 만드는 과정**\n",
    "- 각 단어에 고유 정수 인덱스 부여 (단어 집합 생성)\n",
    "- 각 인덱스의 단어의 등장 횟수 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88507029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "bag of words vector:  [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "    document = document.replace('.', '') # 점 제거\n",
    "    tokenized_document = okt.morphs(document) # 형태소 분석을 통한 토큰화\n",
    "    \n",
    "    word_to_index = {} # 단어집합\n",
    "    bow = []\n",
    "    \n",
    "    for word in tokenized_document:\n",
    "        if word not in word_to_index.keys(): # 처음 보는 단어일 경우\n",
    "            word_to_index[word] = len(word_to_index) # 단어: index\n",
    "            bow.insert(len(word_to_index)-1, 1) # index: 1 (개수)\n",
    "        else: # 이미 본 단어일 경우\n",
    "            index = word_to_index.get(word) # 해당 단어 인덱스 구하기\n",
    "            bow[index] = bow[index]+1 # 해당 인덱스의 개수 +1\n",
    "    return word_to_index, bow\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print('vocabulary: ', vocab)\n",
    "print('bag of words vector: ', bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d868a1",
   "metadata": {},
   "source": [
    "### CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e420428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow:  [[1 1 2 1 2 1]]\n",
      "vocabulary:  {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer() # 길이 2 이상만 토큰으로 인식(정제)\n",
    "\n",
    "print('bow: ', vector.fit_transform(corpus).toarray())\n",
    "print('vocabulary: ', vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2193c0b",
   "metadata": {},
   "source": [
    "### 불용어 제거한 BoW (with CountVectorizer)\n",
    "BoW를 만든다는 것은 문서에서 어떤 단어가 중요한지 빈도수를 통해 알아보겠다는 의미\\\n",
    "따라서 불용어를 제거한 뒤에 BoW를 만들면 성능 상승"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa7082ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92ae34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow:  [[1 1 1 1 1]]\n",
      "vocabulary:  {'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "## 사용자 정의 불용어 사용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vect = CountVectorizer(stop_words=['the', 'a', 'an', 'is', 'not']) # 사용자 정의 불용어 사용\n",
    "# vect = CountVectorizer(stop_words='english') # 제공 불용어 사용\n",
    "print('bow: ', vect.fit_transform(text).toarray())\n",
    "print('vocabulary: ', vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e93bf9",
   "metadata": {},
   "source": [
    "## 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "각 문서를 행으로 하여 여러 문서에 대한 BoW를 하나의 행렬로 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b85298",
   "metadata": {},
   "source": [
    "**한계**\n",
    "- 대부분의 값이 0인 희소표현 => 불용어 제거, 정규화 를 통해 어느 정도 해결\n",
    "- 단순 빈도수 접근에 따른 문서 비교 한계(제거 못한 불용어에 따른 문제)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec712a",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "DTM내에 있는 각 단어에 중요도 계산\\\n",
    "=> 기존 DTM보다 더 많은 정보를 고려하여 문서 비교 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cbd3a1",
   "metadata": {},
   "source": [
    "### tf(d, t): 특정 문서 d에서 특정 단어 t의 등장 횟수\n",
    "DTM[d][t]의 값에 해당"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3001cb",
   "metadata": {},
   "source": [
    "### df(t): 특정 단어 t가 등장한 문서의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debb65d",
   "metadata": {},
   "source": [
    "### idf(d, t): df(t)에 반비례하는 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36d3b",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "tf x idf\\\n",
    "모든 문서에 자주 등장하는 단어는 중요도 낮다고 판단\\\n",
    "특정 문서에서만 자주 등장하는 단어는 중요도 높다고 판단\n",
    "\n",
    "TF-IDF값이 크면 중요도는 높다!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2478e5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray()) # 행은 문서, 열은 단어\n",
    "print(tfidfv.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
