{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16ec925",
   "metadata": {},
   "source": [
    "# 텍스트 전처리(Text preprocessing)\n",
    "\n",
    "데이터를 용도에 맞게 토큰화(tokenization) & 정제(cleaning) & 정규화(normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff65d8df",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)\n",
    "\n",
    "코퍼스(corpus)에서 토큰(token) 단위로 나누는 작업\n",
    "\n",
    "※ token: 의미있는 단위"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef473b12",
   "metadata": {},
   "source": [
    "### -  단어 토큰화(Word Tokenization)\n",
    "\n",
    "영어: 특수기호를 예외처리하여 띄어쓰기 위주로 토큰화\\\n",
    "한글: 형태소를 기준으로 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807448b",
   "metadata": {},
   "source": [
    "### - 품사 태깅(Part-of-speech tagging)\n",
    "\n",
    "품사에 따라 단어의 의미가 달라지기 때문에 품사를 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "992bb856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화:  ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅:  [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## 영어 단어 토큰화 & 품사 태깅\n",
    "from nltk.tokenize import word_tokenize # 단어 토큰화\n",
    "from nltk.tag import pos_tag # 품사태깅\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화: ', tokenized_sentence)\n",
    "print('품사 태깅: ', pos_tag(tokenized_sentence)) # (단어, 품사)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373b7a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석:  ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n",
      "품사 태깅:  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n",
      "명사 추출:  ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "## 한글\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "k_text = \"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"\n",
    "okt = Okt()\n",
    "print('형태소 분석: ', okt.morphs(k_text))\n",
    "print('품사 태깅: ', okt.pos(k_text))\n",
    "print('명사 추출: ', okt.nouns(k_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65dcdb",
   "metadata": {},
   "source": [
    "### - 문장 토큰화(Sentence Tokenization)\n",
    "\n",
    "코퍼스 내에서 문장 단위로 구분\\\n",
    "단순 물음표나 마침표로 구분x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2700d29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(sent_tokenize(text)) # Ph.D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b04dc",
   "metadata": {},
   "source": [
    "### => 여러 토큰화 패키지가 있기에 상황에 맞게 사용!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bc2e1",
   "metadata": {},
   "source": [
    "## 정제(Cleaning) and 정규화(Normalization) \n",
    "\n",
    "정제: 노이즈 데이터(목적에 맞지 않는 단어들) 제거\\\n",
    "ex) 불용어 제거, 등장빈도 적은 단어 제거, 특수문자제거\\\n",
    "\n",
    "정규화: 표현방법이 다른 단어들을 통합시켜서 같은 단어로 만듬\\\n",
    "ex) USA = US, uh-huh = uhhuh, 대소문자 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f26494",
   "metadata": {},
   "source": [
    "### - 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
    "\n",
    "=> 정규화 기법을 통해 문서 내 단어의 수 줄이기\\\n",
    "=> BoW 표현을 사용하는 nlp문제에 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49cc118",
   "metadata": {},
   "source": [
    "### - 표제어 추출(Lemmatization)\n",
    "\n",
    "단어들로 부터 뿌리 단어(표제어)를 찾아가서 단어개수 줄이기\\\n",
    "ex) am, are, is -> be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbadfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전:  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후:  ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', \n",
    "         'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('표제어 추출 전: ', words)\n",
    "print('표제어 추출 후: ', [lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f0fe6d",
   "metadata": {},
   "source": [
    "단어의 형태를 보존하는 양상\\\n",
    "입력 단어의 품사 정보를 알려주면 성능향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3431394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba2b300",
   "metadata": {},
   "source": [
    "=> 품사 정보 보존"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100673ec",
   "metadata": {},
   "source": [
    "### - 어간 추출(Stemming)\n",
    "\n",
    "정해진 규칙만 보고 어림짐작으로 단어의 어미를 자르는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea7f84",
   "metadata": {},
   "source": [
    "**Stemming**\\\n",
    "am → am\\\n",
    "the going → the go\\\n",
    "having → hav\n",
    "\n",
    "**Lemmatization**\\\n",
    "am → be\\\n",
    "the going → the going\\\n",
    "having → have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf0729d",
   "metadata": {},
   "source": [
    "### - 불용어(Stopword)\n",
    "\n",
    "정제작업\\\n",
    "ex) i,my,me,over은 의미분석에 기여x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d549f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수:  179\n",
      "불용어 출력:  ['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # nltk에서 정의한 불용어\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "stop_word_list = stopwords.words('english') # 영어의 불용어 리스트\n",
    "print('불용어 개수: ', len(stop_word_list))\n",
    "print('불용어 출력: ', stop_word_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774fbb66",
   "metadata": {},
   "source": [
    "### - 정규 표현식\n",
    "\n",
    "특정 규칙으로 텍스트 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e97f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "r = re.compile(\"a.c\") # a + 아무거나 + c 형태의 규칙\n",
    "r.search(\"abc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d36c00",
   "metadata": {},
   "source": [
    "## 정수 인코딩(Integer Encoding)\n",
    "\n",
    "텍스트 -> 숫자 과정 전에 각 단어를 고유 정수로 mapping하는 작업\\\n",
    "주로 빈도수 순으로 단어 집합(vocabulary)를 만들고 순서대로 정수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3b5196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63774c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18154352",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(raw_text) # 문장 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28cbdf",
   "metadata": {},
   "source": [
    "텍스트를 수치화 하기 전에 텍스트로 할 수 있는 최대한의 전처리과정\n",
    "\n",
    "1. 정제작업과 정규화 작업 (소문자화, 불용어&단어길이2이하 단어 제외)\n",
    "2. 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbdc27d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences =[]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = word_tokenize(sentence) # 단어 토큰화\n",
    "    result = []\n",
    "    \n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower() # 소문자화(정규화)\n",
    "        \n",
    "        if word not in stop_words: # 불용어면 제거(정제)\n",
    "            if len(word) > 2: # 길이 2 이하면 제거(정제)\n",
    "                result.append(word)\n",
    "                if word not in vocab: # mapping 하기 위한 vocabulary 만들기\n",
    "                    vocab[word] = 0\n",
    "                vocab[word] += 1 # 빈도수에 따른 정렬을 위해 기록\n",
    "    preprocessed_sentences.append(result)\n",
    "print(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58972e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary: ', vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce555ca9",
   "metadata": {},
   "source": [
    "### dictionary 이용해서 정수인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f992054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8),\n",
       " ('secret', 6),\n",
       " ('huge', 5),\n",
       " ('kept', 4),\n",
       " ('person', 3),\n",
       " ('word', 2),\n",
       " ('keeping', 2),\n",
       " ('good', 1),\n",
       " ('knew', 1),\n",
       " ('driving', 1),\n",
       " ('crazy', 1),\n",
       " ('went', 1),\n",
       " ('mountain', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7446ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "## 빈도수 상위 5개 단어만 사용(정제)\n",
    "vocab_size = 5\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, freq) in vocab_sorted[:5]:\n",
    "    i += 1\n",
    "    word_to_index[word] = i\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3824e7d",
   "metadata": {},
   "source": [
    "**Out-Of-Vocabulary(OOV)**: 단어집합에 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f3e41dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'oov': 6}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['oov'] = len(word_to_index) + 1\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af4b208a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 6, 5],\n",
       " [1, 3, 5],\n",
       " [6, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [6, 6, 3, 2, 6, 1, 6],\n",
       " [1, 6, 3, 6]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sentences를 word_to_index를 이용해 mapping\n",
    "encoded_sentences = []\n",
    "for sentence in preprocessed_sentences:\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            encoded_sentence.append(word_to_index[word])\n",
    "        except: # oov일때\n",
    "            encoded_sentence.append(word_to_index['oov'])\n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "encoded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ae71d",
   "metadata": {},
   "source": [
    "### tensorflow 이용해서 정수인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25fba49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "200e6976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['barber', 'person'],\n",
       " ['barber', 'good', 'person'],\n",
       " ['barber', 'huge', 'person'],\n",
       " ['knew', 'secret'],\n",
       " ['secret', 'kept', 'huge', 'secret'],\n",
       " ['huge', 'secret'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'word'],\n",
       " ['barber', 'kept', 'secret'],\n",
       " ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
       " ['barber', 'went', 'huge', 'mountain']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b92101b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barber': 1,\n",
       " 'secret': 2,\n",
       " 'huge': 3,\n",
       " 'kept': 4,\n",
       " 'person': 5,\n",
       " 'word': 6,\n",
       " 'keeping': 7,\n",
       " 'good': 8,\n",
       " 'knew': 9,\n",
       " 'driving': 10,\n",
       " 'crazy': 11,\n",
       " 'went': 12,\n",
       " 'mountain': 13}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "# 빈도수 많은 것만 고려(정제)\n",
    "# padding(index 0)포함 +1\n",
    "tokenizer = Tokenizer(num_words=vocab_size+1)\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "tokenizer.word_index # 빈도수 순으로 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4943ccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('barber', 8),\n",
       "             ('person', 3),\n",
       "             ('good', 1),\n",
       "             ('huge', 5),\n",
       "             ('knew', 1),\n",
       "             ('secret', 6),\n",
       "             ('kept', 4),\n",
       "             ('word', 2),\n",
       "             ('keeping', 2),\n",
       "             ('driving', 1),\n",
       "             ('crazy', 1),\n",
       "             ('went', 1),\n",
       "             ('mountain', 1)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts # 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1737a4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 5],\n",
       " [1, 3, 5],\n",
       " [2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4],\n",
       " [1, 4],\n",
       " [1, 4, 2],\n",
       " [3, 2, 1],\n",
       " [1, 3]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 상위 5개만 단어 인코딩, 나머지는 제거(oov 제거)\n",
    "tokenizer.texts_to_sequences(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbec144",
   "metadata": {},
   "outputs": [],
   "source": [
    "## oov 고려 할경우\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words=vocab_size+2, oov_token='oov') \n",
    "tokenizer.fit_on_texts(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "914aa682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['oov'] # oov의 index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d59e38ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 6],\n",
       " [2, 1, 6],\n",
       " [2, 4, 6],\n",
       " [1, 3],\n",
       " [3, 5, 4, 3],\n",
       " [4, 3],\n",
       " [2, 5, 1],\n",
       " [2, 5, 1],\n",
       " [2, 5, 3],\n",
       " [1, 1, 4, 3, 1, 2, 1],\n",
       " [2, 1, 4, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(preprocessed_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93921ea0",
   "metadata": {},
   "source": [
    "## 패딩(Padding)\n",
    "\n",
    "여러 문장을 동시에 처리하기 위해 각 문자의 길이를 같게 만드는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49530f",
   "metadata": {},
   "source": [
    "### tensorflow로 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bbc3b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5],\n",
       " [1, 8, 5],\n",
       " [1, 3, 5],\n",
       " [9, 2],\n",
       " [2, 4, 3, 2],\n",
       " [3, 2],\n",
       " [1, 4, 6],\n",
       " [1, 4, 6],\n",
       " [1, 4, 2],\n",
       " [7, 7, 3, 2, 10, 1, 11],\n",
       " [1, 12, 3, 13]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0757f9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
       "       [ 0,  0,  0,  0,  1,  8,  5],\n",
       "       [ 0,  0,  0,  0,  1,  3,  5],\n",
       "       [ 0,  0,  0,  0,  0,  9,  2],\n",
       "       [ 0,  0,  0,  2,  4,  3,  2],\n",
       "       [ 0,  0,  0,  0,  0,  3,  2],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  6],\n",
       "       [ 0,  0,  0,  0,  1,  4,  2],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 0,  0,  0,  1, 12,  3, 13]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d48d39bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
       "       [ 1,  8,  5,  0,  0,  0,  0],\n",
       "       [ 1,  3,  5,  0,  0,  0,  0],\n",
       "       [ 9,  2,  0,  0,  0,  0,  0],\n",
       "       [ 2,  4,  3,  2,  0,  0,  0],\n",
       "       [ 3,  2,  0,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  6,  0,  0,  0,  0],\n",
       "       [ 1,  4,  2,  0,  0,  0,  0],\n",
       "       [ 7,  7,  3,  2, 10,  1, 11],\n",
       "       [ 1, 12,  3, 13,  0,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded = pad_sequences(encoded, padding='post') # post: 0을 뒤에 채우기\n",
    "padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9dcdc3",
   "metadata": {},
   "source": [
    "## 원-핫 인코딩(One-Hot Encoding)\n",
    "\n",
    "벡터변환 과정\\\n",
    "그전에 정수인코딩 필수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8a6ee",
   "metadata": {},
   "source": [
    "### tensorflow를 이용한 onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29cd4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45432a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 정수 인코딩\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30292c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 1, 6, 3, 7]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "746ec657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 원핫 인코딩\n",
    "one_hot = to_categorical(encoded)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981f722",
   "metadata": {},
   "source": [
    "**원핫 인코딩 한계**\\\n",
    "차원수 증가, 유사도 표현x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23df1f",
   "metadata": {},
   "source": [
    "## 한국어 전처리 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c4df2",
   "metadata": {},
   "source": [
    "### PyKoSpacing\n",
    "\n",
    "띄어쓰기 되어 있지 않은 문장을 띄어쓰기해주기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e78adc",
   "metadata": {},
   "source": [
    "### Py-Hanspell\n",
    "\n",
    "맞춤법 보정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c447d07",
   "metadata": {},
   "source": [
    "### SOYNLP \n",
    "품사 태깅, 단어 토큰화\\\n",
    "=> 한 단어로 자주 등장하는 것을 학습하여 새로 등장한 단어에 대한 형태소분석 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f7804",
   "metadata": {},
   "source": [
    "### Customized KoNLPy\n",
    "형태소 분석기를 사용하여 단어 토큰화를 할 때 사람이름을 미리 사전에 추가하는 패키지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
