{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b93c099",
   "metadata": {},
   "source": [
    "## 잠재 의미 분석 (Latent Semantic Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113d862c",
   "metadata": {},
   "source": [
    "### 특이값 분해 (Singular Value Decomposition, SVD)\n",
    "\n",
    "$A = U \\sum V^T$\\\n",
    "A : m X n\\\n",
    "U: m X m 직교행렬\\ \n",
    "V: n X n 직교행렬\\\n",
    "$\\sum$ : m X n 직사각 대각행렬 => 대각 원소는 A의 특이값 (Singular value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85771d45",
   "metadata": {},
   "source": [
    "### 절단된 SVD (Truncated SVD)\n",
    "- 대각행렬 시그마의 상위값 t개만 남게 된다.\n",
    "- t는 하이퍼파라미터\n",
    "- t를 크게 잡으면 다양한 의미, 작으면 노이즈 제거\n",
    "![](https://wikidocs.net/images/page/24949/svd%EC%99%80truncatedsvd.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d2a7a",
   "metadata": {},
   "source": [
    "### 잠재 의미 분석 (Latent Semantic Analysis, LSA)\n",
    "- DTM이나 TF-IDF 행렬에 절단된 SVD(truncated SVD)를 사용하여 차원 축소\\\n",
    "    => 잠재적인 의미 이끔\n",
    "- U: 문서개수 X t(topic 개수) => 각 행은 잠재의미를 표현하는 문서벡터\n",
    "- V^T: t(topic개수) X 단어개수 => 각 열은 잠재의미를 표현하는 단어벡터\n",
    "\n",
    "- 단점: SVD 특성상 이미 계산된 LSA에 새로운 데이터 추가하려면 처음부터 다시 계산해야함 (새로운 정보 업데이트 힘듬)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba476d73",
   "metadata": {},
   "source": [
    "## 잠재 디리클레 할당 (Latent Dirichlet Allocation, LDA)\n",
    "- 문서들은 토픽들의 혼합으로 구성, 토픽들은 확률 분포에 기반하여 단어들을 생성 \\\n",
    "=> 데이터가 주어지면 LDA는 생성 과정 역추적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50baa3",
   "metadata": {},
   "source": [
    "### LDA의 가정\n",
    "- 문서 집합으로 부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘\n",
    "- 입력으로 빈도수 기반의 DTM 또는 TF-IDF 사용\n",
    "**문서 생성과정**\n",
    "1. 문서에 사용할 단어 개수 N개 정함\n",
    "2. 문서에 토픽의 혼합을 확률 분포 기반 결정\\\n",
    "ex) topic1 40%, topic2 60%\n",
    "3. 토픽 분포를 통해 토픽을 정하고 토픽에 따른 단어 분포를 통해 단어 결정\n",
    "4. 3번을 단어개수 N개만큼 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8f943",
   "metadata": {},
   "source": [
    "### LDA 수행과정\n",
    "1. 토픽 개수 k 결정 (하이퍼파라미터)\n",
    "2. 모든 단어에 k개중 하나의 토픽 랜덤 할당\\\n",
    "    => 같은 단어에 다른 토픽 할당될 수 있음\n",
    "3. 어떤 문서의 각 단어 w는 자신을 뺀 나머지는 모두 올바른 토픽 할당되었다고 가정하고 두가지 기준으로 재할당 이를 반복\\\n",
    "    => P(topic | document): document의 단어들 중 topic에 해당하는 단어들의 비율\\\n",
    "    => P(word | topic): topic에 해당하는 단어들 중 word의 분포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985a7b6a",
   "metadata": {},
   "source": [
    "## BERT를 이용한 키워드 추출: 키버트(KeyBERT)\n",
    "- 어떤 문서가 입력으로 들어오면 n-gram으로 추출\\\n",
    "    n-gram: 불용어를 제거한 뒤 n 윈도우로 추출\n",
    "- SBERT를 이용해 문서와 각 n-gram 리스트들을 수치화\n",
    "- 코사인 유사도를 통해 top-k개 추출\\\n",
    "    => k개가 다양성이 적다 => 문서를 덜 잘 나타낼 수 있음\n",
    "- 다양성을 높이기 위해 두가지 알고리즘 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6125d2e",
   "metadata": {},
   "source": [
    "### Max Sum Similarity\n",
    "- 문서와 키워드 간의 유사도 최대화\n",
    "- 키워드 간의 유사도는 최소화\n",
    "ex) 문서와 키워드간의 유사도 top10개를 뽑고 그중에서 키워드간 유사도가 낮은 5개 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ca849",
   "metadata": {},
   "source": [
    "### Maximal Marginal Relevance\n",
    "- 텍스트 요약 작업에서 중복을 최소화하고 결과의 다양성 극대화\n",
    "1. 먼저 문서와 가장 유사한 키워드 추출한다\n",
    "2. 문서와 유사하면서 이미 선택된 키워드와 유사하지 않은 새로운 후보를 반복 선택"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
